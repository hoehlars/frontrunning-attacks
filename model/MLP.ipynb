{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Creation - Multi Layer Perceptron\n",
    "Paper:  \n",
    "Sigmoid activation function has been considered in this work\n",
    "as the non-linear transfer function of each neuron. We utilized the\n",
    "Scikit-Learn library [32] in Python to create the model"
   ],
   "id": "49efb62d6d03cc34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Imports:**",
   "id": "6111e58ba935f1a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.292731Z",
     "start_time": "2024-05-04T12:31:36.281442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n"
   ],
   "id": "fc389bab90b9c32f",
   "execution_count": 429,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataset Loading",
   "id": "aeecbc535b43e119"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.323669Z",
     "start_time": "2024-05-04T12:31:36.296122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataset(dataset_files, delimiter=\";\"):\n",
    "  \n",
    "    dataframes = []\n",
    "    data_path = \"../data/\"\n",
    "    \n",
    "    for file in dataset_files:\n",
    "      df = pd.read_csv(data_path+file, delimiter=delimiter)\n",
    "              \n",
    "      label = 0\n",
    "      if \"atk\" in file:\n",
    "        # df.drop(df.columns[0], axis=1, inplace=True) # drop index-column\n",
    "        label = 1\n",
    "      df[\"label\"] = label\n",
    "            \n",
    "      dataframes.append(df)\n",
    "        \n",
    "    concatenated_df = pd.concat(dataframes, axis=0)\n",
    "    concatenated_df.replace({True: 1, False: 0}, inplace=True) # convert boolean to numbers    \n",
    "    concatenated_df.drop(columns=[\"blockNumber\", \"address\", \"transactionHash\"], inplace=True)\n",
    "\n",
    "    # Convert dataframe to torch tensor\n",
    "    data_tensor = torch.tensor(concatenated_df.values, dtype=torch.float)\n",
    "    \n",
    "    # Get the input (data samples) without the target information\n",
    "    X = data_tensor[:, :-1]\n",
    "    \n",
    "    # Get the target information\n",
    "    T = data_tensor[:, -1].unsqueeze(1)\n",
    "\n",
    "    print(\"Features: \", concatenated_df.columns)\n",
    "\n",
    "    print(f\"Loaded dataset with {len(concatenated_df)} samples\")\n",
    "    \n",
    "    return X, T\n"
   ],
   "id": "1247440816defea9",
   "execution_count": 430,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.448857Z",
     "start_time": "2024-05-04T12:31:36.327834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "dataset_files = [\"insertion_atks_first_atk.csv\",\n",
    "                 \"insertion_atks_second_atk.csv\",\n",
    "                 \"random_sampled_transactions.csv\"]\n",
    "\n",
    "X, T = prepare_dataset(dataset_files, \",\")"
   ],
   "id": "830a623bb25040ba",
   "execution_count": 431,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Split Training and Validation Data",
   "id": "d96433c0a7822be7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.464009Z",
     "start_time": "2024-05-04T12:31:36.452114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_training_data(X, T, train_percentage=0.8):\n",
    "  \n",
    "  assert len(X) == len(T)\n",
    "  \n",
    "  num_samples = len(X)\n",
    "  \n",
    "  train_size = int(train_percentage * num_samples)\n",
    "  validation_size = num_samples - train_size\n",
    "    \n",
    "  # split into 80/20 training/validation\n",
    "  idx_train, idx_validation = random_split(range(num_samples), [train_size, validation_size])\n",
    "\n",
    "  \n",
    "  X_train = X[idx_train]\n",
    "  T_train = T[idx_train]\n",
    "  X_val = X[idx_validation]\n",
    "  T_val = T[idx_validation]\n",
    "\n",
    "  return X_train, T_train, X_val, T_val"
   ],
   "id": "4922906edc51f2ef",
   "execution_count": 432,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Input Data Standardization",
   "id": "7ea79b5bf0877476"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.479359Z",
     "start_time": "2024-05-04T12:31:36.467194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standardize(X_train, X_val):\n",
    "  # compute statistics\n",
    "  mean = X_train.mean(dim=0)\n",
    "  std = X_train.std(dim=0)\n",
    "  \n",
    "  torch.save(mean, './mlp-mean.pt')\n",
    "  torch.save(std, './mlp-std.pt')\n",
    "\n",
    "  # Check if standard deviation is zero\n",
    "  zero_std_mask = std == 0\n",
    "  \n",
    "  # Standardize both X_train and X_val, excluding columns with zero standard deviation\n",
    "  X_train = (X_train - mean) / std.masked_fill(zero_std_mask, 1)  # Replace zero std with 1 to avoid division by zero\n",
    "  X_val = (X_val - mean) / std.masked_fill(zero_std_mask, 1)\n",
    "\n",
    "  return X_train, X_val"
   ],
   "id": "e0fbb83915b8a0fa",
   "execution_count": 433,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Network Implementation",
   "id": "5cec002f93d523a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Network Architecture**\n",
    "- 1 input layer\n",
    "  - 7 input neurons\n",
    "- 2 hidden layers\n",
    "  - 10 hidden neurons\n",
    "  - 10 hidden neurons\n",
    "- 1 output layer\n",
    "  - 1 output neuron    \n",
    "\n",
    "Using Sigmoid Activation function!"
   ],
   "id": "795c1456fa4fd486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.495354Z",
     "start_time": "2024-05-04T12:31:36.482354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(7, 10),  # Input layer to first hidden layer\n",
    "            torch.nn.Sigmoid(),  # Sigmoid activation function for the first hidden layer\n",
    "            torch.nn.Linear(10, 10),  # First hidden layer to second hidden layer\n",
    "            torch.nn.Sigmoid(),  # Sigmoid activation function for the second hidden layer\n",
    "            torch.nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "226d8c3436cf6cb3",
   "execution_count": 434,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Training Loop**",
   "id": "ae7c594c844cbeb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.510807Z",
     "start_time": "2024-05-04T12:31:36.498800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(Z, T):\n",
    "  # binary classification\n",
    "  z_sign = (Z >= 0.5).int()\n",
    "  return torch.mean((z_sign == T).float())"
   ],
   "id": "8c6aae3540429565",
   "execution_count": 435,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.526329Z",
     "start_time": "2024-05-04T12:31:36.512801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(network, loss, X_train, T_train, X_val, T_val, num_epochs=10000, learning_rate=0.1):\n",
    "  \n",
    "  optimizer = torch.optim.SGD(\n",
    "    params=network.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=0.9\n",
    "  )\n",
    "\n",
    "  # collect loss and accuracy values\n",
    "  train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # train on training set\n",
    "    optimizer.zero_grad()\n",
    "    # ... compute network output on training data\n",
    "    Z = network(X_train)\n",
    "    \n",
    "    # ... compute loss from network output and target data\n",
    "    J = loss(Z, T_train)\n",
    "    J.backward()\n",
    "    # ... perform parameter update\n",
    "    optimizer.step()\n",
    "    # ... remember loss\n",
    "    train_loss.append(J.item())\n",
    "    # ... compute training set accuracy\n",
    "    train_acc.append(accuracy(Z, T_train).item())\n",
    "    \n",
    "    print(f'Epoch: {epoch}/{num_epochs}')\n",
    "\n",
    "    # test on validation data\n",
    "    with torch.no_grad():\n",
    "      # ... compute network output on validation data\n",
    "      Z = network(X_val)\n",
    "      # ... compute loss from network output and target data\n",
    "      J = loss(Z, T_val)\n",
    "      # ... remember loss\n",
    "      val_loss.append(J.item())\n",
    "      # ... compute validation set accuracy\n",
    "      val_acc.append(accuracy(Z, T_val).item())\n",
    "\n",
    "  # return the four lists of losses and accuracies\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ],
   "id": "9ec7fe03934cf1dc",
   "execution_count": 436,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:31:36.542315Z",
     "start_time": "2024-05-04T12:31:36.528853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot(train_loss, train_acc, val_loss, val_acc):\n",
    "  pyplot.figure(figsize=(10,3))\n",
    "  ax = pyplot.subplot(121)\n",
    "  ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "  ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(122)\n",
    "  ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.legend()"
   ],
   "id": "2bd067c304da38e4",
   "execution_count": 437,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:32:49.086965Z",
     "start_time": "2024-05-04T12:31:36.545556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "#dataset_files = [\"insertion_atks_first_atk_2500.csv\", \"insertion_atks_second_atk_2500.csv\", \"random_sampled_transactions.csv\"]\n",
    "\n",
    "X, T = prepare_dataset(dataset_files, \",\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X, T, 0.8)\n",
    "# Standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "\n",
    "# Initiate the network\n",
    "network = Network()\n",
    "# Define loss function\n",
    "loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "# Train network on our data\n",
    "results = train(network, loss, X_train, T_train, X_val, T_val, num_epochs=10000, learning_rate=0.1)\n",
    "train_loss, train_acc, val_loss, val_acc = results\n",
    "\n",
    "# plot the results\n",
    "plot(train_loss, train_acc, val_loss, val_acc)"
   ],
   "id": "c0571f06178b4e07",
   "execution_count": 438,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:32:49.117677Z",
     "start_time": "2024-05-04T12:32:49.089956Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(network.state_dict(), './front-running-attack-model.pth')",
   "id": "b6c4c7e7e795a5a9",
   "execution_count": 439,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:32:49.133098Z",
     "start_time": "2024-05-04T12:32:49.119685Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape",
   "id": "c657ebf483e778c6",
   "execution_count": 440,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:32:49.148391Z",
     "start_time": "2024-05-04T12:32:49.136098Z"
    }
   },
   "cell_type": "code",
   "source": "print(val_acc[-1])",
   "id": "b382666011d59321",
   "execution_count": 441,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Comparison to other models",
   "id": "63c57353776c4f3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Random Forest**",
   "id": "35406041b5c5f30d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:33:11.066873Z",
     "start_time": "2024-05-04T12:32:49.151395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "classifiers = [KNeighborsClassifier(3),\n",
    "                SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "                SVC(gamma=2, C=1, random_state=42),\n",
    "                #GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "                DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "                RandomForestClassifier(\n",
    "                    max_depth=5, n_estimators=10, max_features=1, random_state=42),\n",
    "                AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "                GaussianNB(),\n",
    "                QuadraticDiscriminantAnalysis(),\n",
    "                MLPClassifier(hidden_layer_sizes=(10,), activation='logistic', solver='adam', max_iter=10000),\n",
    "                ExtraTreeClassifier()]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(classifier)\n",
    "    classifier.fit(X_train, T_train)\n",
    "    predictions = classifier.predict(X_val)\n",
    "    accuracy = accuracy_score(T_val, predictions)\n",
    "    f1 = f1_score(T_val.numpy(), predictions, average='weighted')\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print('--------------------------------')"
   ],
   "id": "290828b9bf8d04b",
   "execution_count": 442,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:33:11.082100Z",
     "start_time": "2024-05-04T12:33:11.069061Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35fb92e6327ccf99",
   "execution_count": 442,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:33:11.098161Z",
     "start_time": "2024-05-04T12:33:11.086160Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d8b9dec19517738",
   "execution_count": 442,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
